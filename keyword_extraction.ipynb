{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebecalozano/keyword-extraction/blob/main/keyword_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33f42b3f",
      "metadata": {
        "id": "33f42b3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df = pd.read_csv('papers.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41b7aa2",
      "metadata": {
        "id": "d41b7aa2",
        "outputId": "edf6d547-bf7e-42a2-89ec-f85da35a1c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc62c21",
      "metadata": {
        "id": "3cc62c21"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a9b289e",
      "metadata": {
        "id": "8a9b289e"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6377692c",
      "metadata": {
        "id": "6377692c"
      },
      "outputs": [],
      "source": [
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d82a3c1",
      "metadata": {
        "id": "4d82a3c1"
      },
      "outputs": [],
      "source": [
        "stop_words = list(stop_words.union(new_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b741bc3",
      "metadata": {
        "id": "3b741bc3"
      },
      "outputs": [],
      "source": [
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    # remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # remove words less than three letters\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "    \n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4286bb",
      "metadata": {
        "id": "4a4286bb"
      },
      "outputs": [],
      "source": [
        "docs = df['paper_text'].apply(lambda x:pre_process(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a8a6012",
      "metadata": {
        "id": "0a8a6012"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588030e9",
      "metadata": {
        "id": "588030e9"
      },
      "outputs": [],
      "source": [
        "#docs = docs.tolist()\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "word_count_vector=cv.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22765282",
      "metadata": {
        "id": "22765282"
      },
      "outputs": [],
      "source": [
        "#Text Frequency Inverse Document Frequency\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75105cfa",
      "metadata": {
        "id": "75105cfa",
        "outputId": "bf36537f-a6ad-4b15-d43a-d59a8af14b7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0f1cb8",
      "metadata": {
        "id": "1d0f1cb8"
      },
      "outputs": [],
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3abe2da",
      "metadata": {
        "id": "a3abe2da"
      },
      "outputs": [],
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "def get_keywords(idx, docs):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1af53537",
      "metadata": {
        "id": "1af53537",
        "outputId": "7c0a4008-58bf-486c-dd73-8e3d3e6c5428"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "Algorithms for Non-negative Matrix Factorization\n",
            "\n",
            "=====Abstract=====\n",
            "Non-negative matrix factorization (NMF) has previously been shown to \r\n",
            "be a useful decomposition for multivariate data. Two different multi- \r\n",
            "plicative algorithms for NMF are analyzed. They differ only slightly in \r\n",
            "the multiplicative factor used in the update rules. One algorithm can be \r\n",
            "shown to minimize the conventional least squares error while the other \r\n",
            "minimizes the generalized Kullback-Leibler divergence. The monotonic \r\n",
            "convergence of both algorithms can be proven using an auxiliary func- \r\n",
            "tion analogous to that used for proving convergence of the Expectation- \r\n",
            "Maximization algorithm. The algorithms can also be interpreted as diag- \r\n",
            "onally rescaled gradient descent, where the rescaling factor is optimally \r\n",
            "chosen to ensure convergence. \n",
            "\n",
            "===Keywords===\n",
            "update rule 0.344\n",
            "update 0.285\n",
            "auxiliary 0.212\n",
            "non negative matrix 0.21\n",
            "negative matrix 0.209\n",
            "rule 0.192\n",
            "nmf 0.183\n",
            "multiplicative 0.175\n",
            "matrix factorization 0.163\n",
            "matrix 0.163\n"
          ]
        }
      ],
      "source": [
        "def print_results(idx, keywords, df):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df['title'][idx])\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(df['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])\n",
        "idx=941\n",
        "keywords=get_keywords(idx, docs)\n",
        "print_results(idx, keywords, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09b0b70",
      "metadata": {
        "id": "e09b0b70"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "keyword extraction.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}